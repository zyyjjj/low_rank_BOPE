{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import botorch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from botorch.acquisition.objective import LearnedObjective\n",
    "from botorch.acquisition.preference import AnalyticExpectedUtilityOfBestOption\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.models.multitask import KroneckerMultiTaskGP\n",
    "from botorch.models.pairwise_gp import (PairwiseGP,\n",
    "                                        PairwiseLaplaceMarginalLogLikelihood)\n",
    "from botorch.models.transforms.input import (InputTransform, ChainedInputTransform,\n",
    "                                             FilterFeatures, Normalize)\n",
    "from botorch.models.transforms.outcome import (OutcomeTransform, ChainedOutcomeTransform,\n",
    "                                               Standardize)\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.posteriors import Posterior, TransformedPosterior\n",
    "\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.test_functions.synthetic import SyntheticTestFunction\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import \\\n",
    "    ExactMarginalLogLikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator for the shapes problem\n",
    "\n",
    "def swap(a,b):\n",
    "    tmp = a\n",
    "    a=b\n",
    "    b=tmp\n",
    "\n",
    "    return a, b\n",
    "\n",
    "class Image(SyntheticTestFunction):\n",
    "    r\"\"\"\n",
    "    Class for generating rectangle images\n",
    "    \"\"\"\n",
    "    dim = 4\n",
    "    _bounds = torch.tensor([[0., 1.], [0., 1.], [0., 1.], [0.,1.]])\n",
    "\n",
    "    def __init__(self, num_pixels: int = 16):\n",
    "        super().__init__()\n",
    "        self.num_pixels = num_pixels\n",
    "        self.pixel_size = 1 / self.num_pixels\n",
    "        self.outcome_dim = num_pixels ** 2\n",
    "    \n",
    "    def evaluate_true(self, X):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            X: 4-dimensional input\n",
    "        Returns:\n",
    "            Y: 256-dimensional array representing 16x16 images\n",
    "        \"\"\"\n",
    "\n",
    "        # map real values in X to integer indices of pixels\n",
    "        pixel_idcs = torch.div(X, self.pixel_size, rounding_mode=\"floor\")\n",
    "\n",
    "        Y = torch.zeros((*X.shape[:-1], self.num_pixels**2))\n",
    "\n",
    "        for sample_idx in range(X.shape[-2]): # TODO: there could be shape erros\n",
    "\n",
    "            row_start, col_start, row_end, col_end = pixel_idcs[sample_idx].numpy().astype(int)\n",
    "\n",
    "            # swap if needed\n",
    "            if row_start > row_end:\n",
    "                row_start, row_end = swap(row_start, row_end)\n",
    "            if col_start > col_end:\n",
    "                col_start, col_end = swap(col_start, col_end)\n",
    "            \n",
    "            paint_it_black = [self.num_pixels * r + c \\\n",
    "                    for r in range(min(row_start, self.num_pixels-1), min(row_end+1, self.num_pixels)) \\\n",
    "                    for c in range(min(col_start, self.num_pixels-1), min(col_end+1, self.num_pixels))]\n",
    "            \n",
    "            Y[sample_idx, paint_it_black] = torch.ones(1, len(paint_it_black))\n",
    "\n",
    "        return Y.to(torch.double)\n",
    "\n",
    "\n",
    "# utility function\n",
    "\n",
    "class AreaUtil(torch.nn.Module):\n",
    "    def __init__(self, weights: Optional[Tensor] = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            weights: `1 x outcome_dim` tensor \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward(self, Y: Tensor):\n",
    "        area = torch.sum(Y, dim = 1)\n",
    "        if self.weights is not None:\n",
    "            area = area * self.weights\n",
    "        return area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTransformedPosterior(TransformedPosterior):\n",
    "    @property\n",
    "    def event_shape(self) -> torch.Size:\n",
    "        r\"\"\"The event shape (i.e. the shape of a single sample).\"\"\"\n",
    "        return self.rsample().shape[-2:]\n",
    "\n",
    "    def _extended_shape(\n",
    "        self, sample_shape: torch.Size = torch.Size()  # noqa: B008\n",
    "    ) -> torch.Size:\n",
    "        r\"\"\"Returns the shape of the samples produced by the posterior with\n",
    "        the given `sample_shape`.\n",
    "\n",
    "        NOTE: This assumes that the `sample_transform` does not change the\n",
    "        shape of the samples.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.rsample().shape[-2:]\n",
    "\n",
    "class PCAOutcomeTransform(OutcomeTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        variance_explained_threshold: float = 0.9,\n",
    "        num_axes: Optional[int] = None,\n",
    "        *tkwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Initialize PCAOutcomeTransform() instance\n",
    "        Args:\n",
    "            variance_explained_threshold: fraction of variance in the data that we want the selected principal axes to explain;\n",
    "                if num_axes is None, use this to decide the number of principal axes to select\n",
    "            num_axes: number of principal axes to select\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.variance_explained_threshold = variance_explained_threshold\n",
    "        self.num_axes = num_axes\n",
    "\n",
    "    def forward(\n",
    "        self, Y: torch.Tensor, Yvar: Optional[torch.Tensor] = None, **tkwargs\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        r\"\"\"\n",
    "        Perform PCA on data Y and transforms it to a lower-dimensional representation in terms of principal components.\n",
    "        Args:\n",
    "            Y: `batch_shape x num_samples x output_dim` tensor of metric observations;\n",
    "                assume that it is centered already, since we would normally chain the PCAOutcomeTransform() after a Standardize()\n",
    "            Yvar: (optional) `batch_shape x num_samples x output_dim` tensor of metric noises (variance)\n",
    "        Returns:\n",
    "            Y_transformed: `batch_shape x num_samples x PC_dim` tensor of PC values\n",
    "            Yvar_transformed: `batch_shape x num_samples x PC_dim` tensor of estimated PC variances\n",
    "        \"\"\"\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            U, S, V = torch.svd(Y)\n",
    "            S_squared = torch.square(S)\n",
    "            explained_variance = S_squared / S_squared.sum()\n",
    "\n",
    "            if self.num_axes is None:\n",
    "                # decide the number of principal axes to keep (that makes explained variance exceed the specified threshold)\n",
    "                exceed_thres = (\n",
    "                    np.cumsum(explained_variance) > self.variance_explained_threshold\n",
    "                )\n",
    "                self.num_axes = len(exceed_thres) - sum(exceed_thres) + 1\n",
    "\n",
    "            axes_learned = torch.transpose(V[:, : self.num_axes], -2, -1)\n",
    "            self.PCA_explained_variance = sum(explained_variance[: self.num_axes])\n",
    "            self.axes_learned = torch.tensor(axes_learned, **tkwargs)\n",
    "\n",
    "        Y_transformed = torch.matmul(Y, torch.transpose(self.axes_learned, -2, -1)).to(\n",
    "            **tkwargs\n",
    "        )\n",
    "\n",
    "        # if Yvar is given, the variance of PCs is lower bounded by the linear combination of Yvar terms\n",
    "        # technically, the variance of PCs should include the covariance between Y's, but that is usually not available\n",
    "        if Yvar is not None:\n",
    "            Yvar_transformed = torch.matmul(\n",
    "                Yvar, torch.square(torch.transpose(self.axes_learned, -2, -1))\n",
    "            ).to(**tkwargs)\n",
    "\n",
    "        return Y_transformed, Yvar_transformed if Yvar is not None else None\n",
    "\n",
    "    def untransform(\n",
    "        self, Y: torch.Tensor, Yvar: Optional[torch.Tensor] = None, **tkwargs\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        r\"\"\"\n",
    "        Transform PC back to metrics according to self.axes_learned.\n",
    "        Args:\n",
    "            Y: `num_samples x PC_dim` tensor of PC values\n",
    "            Yvar: `num_samples x PC_dim` tensor of PC variances\n",
    "        Returns:\n",
    "            Y_untransformed: `num_samples x output_dim` tensor of metric values\n",
    "            Yvar_untransformed: `num_samples x output_dim` tensor of metric variances\n",
    "        \"\"\"\n",
    "\n",
    "        Y_untransformed = torch.matmul(Y, self.axes_learned)\n",
    "        if Yvar is not None:\n",
    "            Yvar_untransformed = torch.matmul(Yvar, torch.square(self.axes_learned))\n",
    "\n",
    "        return (\n",
    "            Y_untransformed,\n",
    "            Yvar_untransformed if Yvar is not None else None,\n",
    "        )\n",
    "\n",
    "    def untransform_posterior(self, posterior: Posterior):\n",
    "        r\"\"\"\n",
    "        Create posterior distribution in the space of metrics.\n",
    "        Args:\n",
    "            posterior: posterior in the space of PCs\n",
    "        Returns:\n",
    "            untransformed_posterior: posterior in the space of metrics\n",
    "        \"\"\"\n",
    "\n",
    "        untransformed_posterior = ModifiedTransformedPosterior(\n",
    "            posterior=posterior,\n",
    "            sample_transform=lambda x: x.matmul(self.axes_learned),\n",
    "            mean_transform=lambda x, v: x.matmul(self.axes_learned),\n",
    "            variance_transform=lambda x, v: v.matmul(torch.square(self.axes_learned)),\n",
    "        )\n",
    "\n",
    "        return untransformed_posterior\n",
    "\n",
    "\n",
    "class PCAInputTransform(InputTransform, torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        axes: torch.Tensor,\n",
    "        transform_on_train: bool = True,\n",
    "        transform_on_eval: bool = True,\n",
    "        transform_on_fantasize: bool = True,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Initialize PCAInputTransform() instance.\n",
    "        Args:\n",
    "            axes: `num_axes x input_dim` tensor with norm-1 orthogonal rows\n",
    "                (in the case of PE, these are the principal axes\n",
    "                learned from the previous stage of fitting outcome model)\n",
    "            transform_on_train: A boolean indicating whether to apply the\n",
    "                transform in train() mode.\n",
    "            transform_on_eval: A boolean indicating whether to apply the\n",
    "                transform in eval() mode.\n",
    "            transform_on_fantasize: A boolean indicating whether to apply\n",
    "                the transform when called from within a `fantasize` call.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.axes = axes\n",
    "        self.transform_on_train = transform_on_train\n",
    "        self.transform_on_eval = transform_on_eval\n",
    "        self.transform_on_fantasize = transform_on_fantasize\n",
    "\n",
    "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Transform the input X into latent representations using self.axes.\n",
    "        Args:\n",
    "            X: `num_samples x input_dim` tensor of input data\n",
    "        \"\"\"\n",
    "\n",
    "        transformed_X = torch.matmul(X, torch.transpose(self.axes, -2, -1))\n",
    "\n",
    "        return transformed_X\n",
    "\n",
    "    def untransform(self, X_tf: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Untransform a latent representation back to input space.\n",
    "        Args:\n",
    "            X_tf: `num_samples x num_axes` tensor of latent representations\n",
    "        \"\"\"\n",
    "\n",
    "        untransformed_X = torch.matmul(X_tf, self.axes)\n",
    "\n",
    "        return untransformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.priors.torch_priors import GammaPrior\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.priors import SmoothedBoxPrior\n",
    "from gpytorch.constraints import GreaterThan, Interval\n",
    "from botorch.exceptions.errors import BotorchTensorDimensionError\n",
    "\n",
    "from botorch.models.transforms.input import InputTransform, ReversibleInputTransform\n",
    "\n",
    "\n",
    "# modified kernel with change in hyperpriors\n",
    "def make_modified_kernel(ard_num_dims, a=0.2, b=5.0):\n",
    "    ls_prior = GammaPrior(1.2, 0.5)\n",
    "    ls_prior_mode = (ls_prior.concentration - 1) / ls_prior.rate\n",
    "\n",
    "    covar_module = ScaleKernel(\n",
    "        RBFKernel(\n",
    "            ard_num_dims=ard_num_dims,\n",
    "            lengthscale_prior=ls_prior,\n",
    "            lengthscale_constraint=GreaterThan(\n",
    "                lower_bound=1e-4, transform=None, initial_value=ls_prior_mode\n",
    "            ),\n",
    "        ),\n",
    "        outputscale_prior=SmoothedBoxPrior(a=a, b=b),\n",
    "        outputscale_constraint=Interval(lower_bound=a, upper_bound=b),\n",
    "        # outputscale_prior=SmoothedBoxPrior(a=1e-2, b=1e2),\n",
    "        # outputscale_prior=SmoothedBoxPrior(a=0.1, b=10),\n",
    "        # outputscale_constraint=Interval(lower_bound=0.1, upper_bound=10),\n",
    "    )\n",
    "\n",
    "    return covar_module\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InputCenter(ReversibleInputTransform, torch.nn.Module):\n",
    "    r\"\"\"Center the inputs (zero mean), don't change the variance.\n",
    "    This class is modified from InputStandardize.\n",
    "\n",
    "    In train mode, calling `forward` updates the module state\n",
    "    (i.e. the mean/std normalizing constants). If in eval mode, calling `forward`\n",
    "    simply applies the standardization using the current module state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d: int,\n",
    "        indices: Optional[List[int]] = None,\n",
    "        batch_shape: torch.Size = torch.Size(),  # noqa: B008\n",
    "        transform_on_train: bool = True,\n",
    "        transform_on_eval: bool = True,\n",
    "        transform_on_fantasize: bool = True,\n",
    "        reverse: bool = False,\n",
    "        min_std: float = 1e-8,\n",
    "    ) -> None:\n",
    "        r\"\"\"Center inputs (zero mean).\n",
    "\n",
    "        Args:\n",
    "            d: The dimension of the input space.\n",
    "            indices: The indices of the inputs to standardize. If omitted,\n",
    "                take all dimensions of the inputs into account.\n",
    "            batch_shape: The batch shape of the inputs (asssuming input tensors\n",
    "                of shape `batch_shape x n x d`). If provided, perform individual\n",
    "                normalization per batch, otherwise uses a single normalization.\n",
    "            transform_on_train: A boolean indicating whether to apply the\n",
    "                transforms in train() mode. Default: True\n",
    "            transform_on_eval: A boolean indicating whether to apply the\n",
    "                transform in eval() mode. Default: True\n",
    "            reverse: A boolean indicating whether the forward pass should untransform\n",
    "                the inputs.\n",
    "            min_std: Amount of noise to add to the standard deviation to ensure no\n",
    "                division by zero errors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if (indices is not None) and (len(indices) == 0):\n",
    "            raise ValueError(\"`indices` list is empty!\")\n",
    "        if (indices is not None) and (len(indices) > 0):\n",
    "            indices = torch.tensor(indices, dtype=torch.long)\n",
    "            if len(indices) > d:\n",
    "                raise ValueError(\"Can provide at most `d` indices!\")\n",
    "            if (indices > d - 1).any():\n",
    "                raise ValueError(\"Elements of `indices` have to be smaller than `d`!\")\n",
    "            if len(indices.unique()) != len(indices):\n",
    "                raise ValueError(\"Elements of `indices` tensor must be unique!\")\n",
    "            self.indices = indices\n",
    "        self.register_buffer(\"means\", torch.zeros(*batch_shape, 1, d))\n",
    "        self.register_buffer(\"stds\", torch.ones(*batch_shape, 1, d))\n",
    "        self._d = d\n",
    "        self.transform_on_train = transform_on_train\n",
    "        self.transform_on_eval = transform_on_eval\n",
    "        self.transform_on_fantasize = transform_on_fantasize\n",
    "        self.batch_shape = batch_shape\n",
    "        self.min_std = min_std\n",
    "        self.reverse = reverse\n",
    "        self.learn_bounds = True\n",
    "\n",
    "    def _transform(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Center the inputs.\n",
    "\n",
    "        In train mode, calling `forward` updates the module state\n",
    "        (i.e. the mean/std normalizing constants). If in eval mode, calling `forward`\n",
    "        simply applies the standardization using the current module state.\n",
    "\n",
    "        Args:\n",
    "            X: A `batch_shape x n x d`-dim tensor of inputs.\n",
    "\n",
    "        Returns:\n",
    "            A `batch_shape x n x d`-dim tensor of inputs, de-meaned.\n",
    "        \"\"\"\n",
    "        if self.training and self.learn_bounds:\n",
    "            if X.size(-1) != self.means.size(-1):\n",
    "                raise BotorchTensorDimensionError(\n",
    "                    f\"Wrong input. dimension. Received {X.size(-1)}, \"\n",
    "                    f\"expected {self.means.size(-1)}\"\n",
    "                )\n",
    "\n",
    "            n = len(self.batch_shape) + 2\n",
    "            if X.ndim < n:\n",
    "                raise ValueError(\n",
    "                    f\"`X` must have at least {n} dimensions, {n - 2} batch and 2 innate\"\n",
    "                    f\" , but has {X.ndim}.\"\n",
    "                )\n",
    "\n",
    "            # Aggregate means and standard deviations over extra batch and marginal dims\n",
    "            batch_ndim = min(len(self.batch_shape), X.ndim - 2)  # batch rank of `X`\n",
    "            reduce_dims = (*range(X.ndim - batch_ndim - 2), X.ndim - 2)\n",
    "            self.stds, self.means = (\n",
    "                values.unsqueeze(-2)\n",
    "                for values in torch.std_mean(X, dim=reduce_dims, unbiased=True)\n",
    "            )\n",
    "            self.stds.clamp_(min=self.min_std)\n",
    "\n",
    "        if hasattr(self, \"indices\"):\n",
    "            X_new = X.clone()\n",
    "            X_new[..., self.indices] = (\n",
    "                X_new[..., self.indices] - self.means[..., self.indices]\n",
    "            )\n",
    "\n",
    "            return X_new\n",
    "\n",
    "        return X - self.means\n",
    "\n",
    "    def _untransform(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Un-center the inputs, i.e., add back the mean.\n",
    "\n",
    "        Args:\n",
    "            X: A `batch_shape x n x d`-dim tensor of centered inputs.\n",
    "\n",
    "        Returns:\n",
    "            A `batch_shape x n x d`-dim tensor of un-centered inputs.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"indices\"):\n",
    "            X_new = X.clone()\n",
    "            X_new[..., self.indices] = (\n",
    "                self.means[..., self.indices] + X_new[..., self.indices]\n",
    "            )\n",
    "\n",
    "            return X_new\n",
    "\n",
    "        return self.means.to(X) + X\n",
    "\n",
    "    def equals(self, other: InputTransform) -> bool:\n",
    "        r\"\"\"Check if another input transform is equivalent.\n",
    "\n",
    "        Args:\n",
    "            other: Another input transform.\n",
    "\n",
    "        Returns:\n",
    "            A boolean indicating if the other transform is equivalent.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"indices\") == hasattr(other, \"indices\"):\n",
    "            if hasattr(self, \"indices\"):\n",
    "                return (\n",
    "                    super().equals(other=other)\n",
    "                    and (self._d == other._d)\n",
    "                    and (self.indices == other.indices).all()\n",
    "                )\n",
    "            else:\n",
    "                return super().equals(other=other) and (self._d == other._d)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition.objective import MCAcquisitionObjective\n",
    "from botorch.models.model import Model\n",
    "from botorch.acquisition.monte_carlo import (qNoisyExpectedImprovement,\n",
    "                                             qSimpleRegret)\n",
    "\n",
    "def generate_random_inputs(problem: torch.nn.Module, n: int) -> Tensor:\n",
    "    r\"\"\"Generate n quasi-random Sobol points in the design space.\n",
    "    Args:\n",
    "        problem: a TestProblem in Botorch\n",
    "        n: number of random inputs to generate\n",
    "    Returns:\n",
    "        `n x problem input dim` tensor of randomly generated points in problem's input domain\n",
    "    \"\"\"\n",
    "    return (\n",
    "        draw_sobol_samples(bounds=problem.bounds, n=1, q=n).squeeze(0).to(torch.double)\n",
    "    )\n",
    "\n",
    "\n",
    "def gen_comps(\n",
    "    util_vals: Tensor, comp_noise_type: str = None, comp_noise: float = None\n",
    ") -> Tensor:\n",
    "    r\"\"\"Create pairwise comparisons.\n",
    "    Args:\n",
    "        util_vals: `num_outcomes x 1` tensor of utility values\n",
    "        comp_noise_type: type of comparison noise to inject, one of {'constant', 'probit'}\n",
    "        comp_noise: parameter related to probability of making a comparison mistake\n",
    "    Returns:\n",
    "        comp_pairs: `(num_outcomes // 2) x 2` tensor showing the preference,\n",
    "            with the more preferable outcome followed by the other one in each row\n",
    "    \"\"\"\n",
    "    cpu_util = util_vals.cpu()\n",
    "\n",
    "    comp_pairs = []\n",
    "    for i in range(cpu_util.shape[0] // 2):\n",
    "        i1 = i * 2\n",
    "        i2 = i * 2 + 1\n",
    "        if cpu_util[i1] > cpu_util[i2]:\n",
    "            new_comp = [i1, i2]\n",
    "            util_diff = cpu_util[i1] - cpu_util[i2]\n",
    "        else:\n",
    "            new_comp = [i2, i1]\n",
    "            util_diff = cpu_util[i2] - cpu_util[i1]\n",
    "\n",
    "        new_comp = torch.tensor(new_comp, device=util_vals.device, dtype=torch.long)\n",
    "        # if comp_noise_type is not None:\n",
    "        #     new_comp = inject_comp_error(\n",
    "        #         new_comp, util_diff, comp_noise_type, comp_noise\n",
    "        #     )\n",
    "        comp_pairs.append(new_comp)\n",
    "\n",
    "    comp_pairs = torch.stack(comp_pairs)\n",
    "\n",
    "    return comp_pairs\n",
    "\n",
    "\n",
    "def gen_initial_real_data(\n",
    "    n: int, problem: torch.nn.Module, util_func: torch.nn.Module, comp_noise: float = 0, batch_eval: bool = True\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    # generate (noisy) ground truth data\n",
    "    r\"\"\"Generate noisy ground truth inputs, outcomes, utility values, and comparisons.\n",
    "    Args:\n",
    "        n: number of samples to generate\n",
    "        problem: a TestProblem\n",
    "        util_func: ground truth utility function (outcome -> utility)\n",
    "        comp_noise: noise to inject into the comparisons\n",
    "    Returns:\n",
    "        X: generated inputs\n",
    "        Y: generated (noisy) outcomes from evaluating the problem on X\n",
    "        util_vals: utility values of generated Y\n",
    "        comps: comparison results for adjacent pairs of util_vals\n",
    "    \"\"\"\n",
    "\n",
    "    X = generate_random_inputs(problem, n).detach()\n",
    "\n",
    "    if not batch_eval:\n",
    "        Y_list = []\n",
    "        for idx in range(len(X)):\n",
    "            y = problem(X[idx]).detach()\n",
    "            Y_list.append(y)\n",
    "        Y = torch.stack(Y_list).squeeze(1)\n",
    "    else:\n",
    "        Y = problem(X).detach()\n",
    "\n",
    "    util_vals = util_func(Y).detach()\n",
    "    # comps = gen_comps(util_vals, comp_noise_type=\"constant\", comp_noise=comp_noise)\n",
    "    comps = gen_comps(util_vals)\n",
    "\n",
    "    return X, Y, util_vals, comps\n",
    "\n",
    "\n",
    "\n",
    "def find_true_optimal_utility(\n",
    "    problem: torch.nn.Module, \n",
    "    util_func: torch.nn.Module, \n",
    "    n: int,\n",
    "    maximize: bool = True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Find the optimal utility value, i.e., max_x (util_func(problem(x))) across\n",
    "    the domain through taking a large number of samples.\n",
    "    Args:\n",
    "        problem: a TestProblem that maps designs to outcomes\n",
    "        util_func: maps outcomes to scalar utility\n",
    "        n: number of evalutions\n",
    "        maximize: boolean for whether to maximize (if False, minimize)\n",
    "    \"\"\"\n",
    "\n",
    "    meta_batch_size = 20000 // problem.dim\n",
    "    num_meta_batches = n // meta_batch_size + 1\n",
    "    best_util_vals = []\n",
    "\n",
    "    for _ in range(num_meta_batches):\n",
    "        _, _, util_vals, _ = gen_initial_real_data(\n",
    "            n=meta_batch_size,\n",
    "            problem=problem,\n",
    "            util_func=util_func,\n",
    "            comp_noise=0,\n",
    "            batch_eval=True\n",
    "        )\n",
    "        if maximize:\n",
    "            best_util_vals.append(torch.max(util_vals).item())\n",
    "        else:\n",
    "            best_util_vals.append(torch.min(util_vals).item())\n",
    "    \n",
    "    if maximize:\n",
    "        return np.max(best_util_vals)\n",
    "    else:\n",
    "        return np.min(best_util_vals)\n",
    "\n",
    "\n",
    "\n",
    "def gen_exp_cand(\n",
    "    outcome_model: Model,\n",
    "    objective: MCAcquisitionObjective,\n",
    "    problem: torch.nn.Module,\n",
    "    q: int,\n",
    "    acqf_name: str,\n",
    "    seed: int,\n",
    "    X: Optional[Tensor] = None,\n",
    "    sampler_num_outcome_samples: int = 64,\n",
    "    num_restarts: int = 8,\n",
    "    raw_samples: int = 64,\n",
    "    batch_limit: int = 4,\n",
    ") -> Tensor:\n",
    "    \"\"\"Given an outcome model and an objective, generate q experimental candidates\n",
    "    using a specified acquisition function.\n",
    "    Args:\n",
    "        outcome_model: GP model mapping input to outcome\n",
    "        objective: MC objective mapping outcome to utility\n",
    "        problem: a TestProblem\n",
    "        q: number of candidates to generate\n",
    "        acqf_name: name of acquisition function, one of {'qNEI', 'posterior_mean'}\n",
    "        X: `num_outcome_samples x input_dim` current training data\n",
    "        sampler_num_outcome_samples: number of base samples in acq function's sampler\n",
    "        num_restarts: number of starting points for multi-start acqf optimization\n",
    "        raw_samples: number of samples for initializing acqf optimization\n",
    "        batch_limit: the limit on batch size in gen_candidates_scipy() within optimize_acqf()\n",
    "    Returns:\n",
    "        candidates: `q x problem input dim` generated candidates\n",
    "    \"\"\"\n",
    "    sampler = SobolQMCNormalSampler(sampler_num_outcome_samples)\n",
    "    if acqf_name == \"qNEI\":\n",
    "        # generate experimental candidates with qNEI/qNEIUU\n",
    "        acq_func = qNoisyExpectedImprovement(\n",
    "            model=outcome_model,\n",
    "            objective=objective,\n",
    "            X_baseline=X,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "            cache_root=False,\n",
    "        )\n",
    "    elif acqf_name == \"posterior_mean\":\n",
    "        # generate experimental candidates with maximum posterior mean\n",
    "        acq_func = qSimpleRegret(\n",
    "            model=outcome_model,\n",
    "            sampler=sampler,\n",
    "            objective=objective,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown acquisition function name!\")\n",
    "\n",
    "    # optimize the acquisition function\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        q=q,\n",
    "        bounds=problem.bounds,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options={\"batch_limit\": batch_limit, \"seed\": seed},\n",
    "        sequential=True,\n",
    "    )\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.model import Model\n",
    "from botorch.models.deterministic import DeterministicModel\n",
    "\n",
    "\n",
    "class ModifiedFixedSingleSampleModel(DeterministicModel):\n",
    "    r\"\"\"\n",
    "    A deterministic model defined by a single sample `w`.\n",
    "\n",
    "    Given a base model `f` and a fixed sample `w`, the model always outputs\n",
    "\n",
    "        y = f_mean(x) + f_stddev(x) * w\n",
    "\n",
    "    We assume the outcomes are uncorrelated here.\n",
    "\n",
    "    This is modified from FixedSingleSampleModel to handle dimensionality reduction.\n",
    "    For models with dim reduction, model.num_outputs is the reduced outcome dimension,\n",
    "    whereas we want w to be in the original outcome dimension.\n",
    "    In this modification, we define self.w within forward() rather than __init__(),\n",
    "    where we fix the dimensionality of w to be posterior(X).event_shape[-1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: Model, outcome_dim: int, w: Optional[torch.Tensor] = None\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            model: The base model.\n",
    "            outcome_dim: dimensionality of the outcome space\n",
    "            w: A 1-d tensor with length = outcome_dim.\n",
    "                If None, draw it from a standard normal distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w = torch.randn(outcome_dim)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        post = self.model.posterior(X)\n",
    "\n",
    "        return post.mean + post.variance.sqrt() * self.w.to(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "def fit_outcome_model(X: torch.Tensor, Y: torch.Tensor, **model_kwargs) -> Model:\n",
    "    r\"\"\"Fit outcome model.\n",
    "    Args:\n",
    "        X: `num_samples x input_dim` input data\n",
    "        Y: `num_samples x outcome_dim` outcome data\n",
    "        model_kwargs: arguments for fitting outcome GP,\n",
    "            such as outcome_transform, covar_module, likelihood, etc.\n",
    "    Returns:\n",
    "        outcome_model: Fitted outcome model mapping input to outcome\n",
    "    \"\"\"\n",
    "\n",
    "    outcome_model = SingleTaskGP(train_X=X, train_Y=Y, **model_kwargs)\n",
    "\n",
    "    mll_outcome = ExactMarginalLogLikelihood(outcome_model.likelihood, outcome_model)\n",
    "    # fit_gpytorch_model(mll_outcome)\n",
    "    fit_gpytorch_mll(mll_outcome)\n",
    "\n",
    "    return outcome_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOPE code\n",
    "\n",
    "\n",
    "class BopeExperiment:\n",
    "\n",
    "    attr_list = {\n",
    "        \"pca_var_threshold\": 0.95,\n",
    "        \"initial_experimentation_batch\": 16,\n",
    "        \"n_check_post_mean\": 20,\n",
    "        \"every_n_comps\": 3,\n",
    "        \"verbose\": True,\n",
    "        \"dtype\": torch.double,\n",
    "        \"noise_std\": 0.01,  # TODO: figure out how to set this for different probs\n",
    "        \"num_restarts\": 20,\n",
    "        \"raw_samples\": 128,\n",
    "        \"batch_limit\": 4,\n",
    "        \"sampler_num_outcome_samples\": 64,\n",
    "        \"maxiter\": 1000,\n",
    "        \"latent_dim\": None,\n",
    "        \"min_stdv\": 100000,\n",
    "        \"true_axes\": None, # specify these for synthetic problems\n",
    "        \"save\": False\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem: torch.nn.Module,\n",
    "        util_func: torch.nn.Module,\n",
    "        methods: List[str],\n",
    "        pe_strategies: List[str],\n",
    "        trial_idx: int,\n",
    "        output_path: Optional[str]=None,\n",
    "        save: bool =False,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        specify experiment settings\n",
    "            problem:\n",
    "            util_func:\n",
    "            methods: list of statistical methods, each denoted using a str\n",
    "            pe_strategies: PE strategies to use, could be {\"EUBO-zeta\", \"Random-f\"}\n",
    "        one run should handle one problem and >=1 methods and >=1 pe_strategies\n",
    "        \"\"\"\n",
    "\n",
    "        # self.attr_list stores default values, then overwrite with kwargs\n",
    "        for key in self.attr_list.keys():\n",
    "            setattr(self, key, self.attr_list[key])\n",
    "        for key in kwargs.keys():\n",
    "            setattr(self, key, kwargs[key])\n",
    "        \n",
    "        print(\"BopeExperiment settings: \", self.attr_list)\n",
    "\n",
    "        # pre-specified experiment metadata\n",
    "        self.problem = problem\n",
    "        self.util_func = util_func\n",
    "        self.pe_strategies = pe_strategies\n",
    "        self.outcome_dim = problem.outcome_dim\n",
    "        self.input_dim = problem._bounds.shape[-1]\n",
    "        self.trial_idx = trial_idx\n",
    "        self.output_path = output_path\n",
    "        if save:\n",
    "            if not os.path.exists(self.output_path):\n",
    "                os.makedirs(self.output_path)\n",
    "        if hasattr(self.problem, \"true_axes\"):\n",
    "            self.true_axes = self.problem.true_axes\n",
    "\n",
    "        if \"pca\" in methods:\n",
    "            # make sure to run pca first, so that the learned latent_dim\n",
    "            # informs the other methods with dim reduction\n",
    "            self.methods = [\"pca\"] + [m for m in methods if m != \"pca\"]\n",
    "            print('self.methods, ', self.methods)\n",
    "        else:\n",
    "            # if \"pca\" is not run and latent_dim is not specified\n",
    "            # set self.latent_dim by hand\n",
    "            self.methods = methods\n",
    "            if not self.latent_dim:\n",
    "                self.latent_dim = self.outcome_dim // 3\n",
    "\n",
    "        # logging models and results\n",
    "        self.outcome_models_dict = {}  # by method\n",
    "        self.pref_data_dict = defaultdict(dict)  # by (method, pe_strategy)\n",
    "        self.PE_time_dict = defaultdict(dict)\n",
    "        self.PE_session_results = defaultdict(dict) # deps on method and pe strategy\n",
    "        self.final_candidate_results = defaultdict(dict) # deps on method and pe strategy\n",
    "        self.outcome_model_fitting_results = defaultdict(dict) # only deps on method\n",
    "\n",
    "        # specify outcome and input transforms, covariance modules\n",
    "        self.transforms_covar_dict = {\n",
    "            \"st\": {\n",
    "                \"outcome_tf\": Standardize(self.outcome_dim),\n",
    "                \"input_tf\": Normalize(self.outcome_dim),\n",
    "                \"covar_module\": make_modified_kernel(ard_num_dims=self.outcome_dim),\n",
    "            },\n",
    "            \"pca\": {\n",
    "                \"outcome_tf\": ChainedOutcomeTransform(\n",
    "                    **{\n",
    "                        \"standardize\": Standardize(\n",
    "                            self.outcome_dim,\n",
    "                            min_stdv=self.min_stdv,\n",
    "                        ),\n",
    "                        \"pca\": PCAOutcomeTransform(\n",
    "                            variance_explained_threshold=self.pca_var_threshold,\n",
    "                            num_axes=self.latent_dim,\n",
    "                        ),\n",
    "                    }\n",
    "                ),\n",
    "            },            \n",
    "        }\n",
    "        \n",
    "        # compute true optimal utility\n",
    "        self.true_opt = find_true_optimal_utility(self.problem, self.util_func, n=5000)\n",
    "\n",
    "    def generate_random_experiment_data(self, n, compute_util: False):\n",
    "        r\"\"\"Generate n observations of experimental designs and outcomes.\n",
    "        Args:\n",
    "            problem: a TestProblem\n",
    "            n: number of samples\n",
    "        Computes:\n",
    "            X: `n x problem input dim` tensor of sampled inputs\n",
    "            Y: `n x problem outcome dim` tensor of noisy evaluated outcomes at X\n",
    "            util_vals: `n x 1` tensor of utility value of Y\n",
    "            comps: `n/2 x 2` tensor of pairwise comparisons \n",
    "        \"\"\"\n",
    "\n",
    "        self.X = (\n",
    "            draw_sobol_samples(bounds=self.problem.bounds, n=1, q=n)\n",
    "            .squeeze(0)\n",
    "            .to(torch.double)\n",
    "            .detach()\n",
    "        )\n",
    "        self.Y = self.problem(self.X).detach()\n",
    "\n",
    "        if compute_util:\n",
    "            self.util_vals = self.util_func(self.Y).detach()\n",
    "            self.comps = gen_comps(self.util_vals)\n",
    "\n",
    "    def fit_outcome_model(self, method):\n",
    "        r\"\"\"Fit outcome model based on specified method.\n",
    "        Args:\n",
    "            method: string specifying the statistical model\n",
    "        \"\"\"\n",
    "        print(f\"Fitting outcome model using {method}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        outcome_model = fit_outcome_model(\n",
    "            self.X,\n",
    "            self.Y,\n",
    "            outcome_transform=self.transforms_covar_dict[method][\"outcome_tf\"],\n",
    "        )\n",
    "        \n",
    "        model_fitting_time = time.time() - start_time\n",
    "        # rel_mse = check_outcome_model_fit(outcome_model, self.problem, n_test=1000)\n",
    "        # self.outcome_model_fitting_results[method] = {\n",
    "        #     \"model_fitting_time\": model_fitting_time,\n",
    "        #     \"rel_mse\": rel_mse\n",
    "        # }\n",
    "\n",
    "        if method == \"pca\":\n",
    "            self.pca_axes = outcome_model.outcome_transform[\"pca\"].axes_learned\n",
    "            self.transforms_covar_dict[\"pca\"][\"input_tf\"] = ChainedInputTransform(\n",
    "                **{\n",
    "                    # \"standardize\": InputStandardize(config[\"outcome_dim\"]),\n",
    "                    # TODO: was trying standardize again\n",
    "                    \"center\": InputCenter(self.outcome_dim),\n",
    "                    \"pca\": PCAInputTransform(axes=self.pca_axes),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            self.latent_dim = self.pca_axes.shape[0]\n",
    "            print(\n",
    "                f\"amount of variance explained by {self.latent_dim} axes: {outcome_model.outcome_transform['pca'].PCA_explained_variance}\"\n",
    "            )\n",
    "            self.outcome_model_fitting_results[method].update(\n",
    "                {\"num_pca_axes\": self.latent_dim}\n",
    "            )\n",
    "\n",
    "            # here we first see how many latent dimensions PCA learn\n",
    "            # then create a random linear proj onto the same dimensionality\n",
    "            # similarly, set that as the cardinality of the random subset \n",
    "            self.transforms_covar_dict[\"pca\"][\"covar_module\"] = make_modified_kernel(\n",
    "                ard_num_dims=self.latent_dim\n",
    "            )\n",
    "\n",
    "        self.outcome_models_dict[method] = outcome_model\n",
    "\n",
    "    def fit_pref_model(\n",
    "        self,\n",
    "        Y,\n",
    "        comps,\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        r\"\"\"Fit utility model based on given data and model_kwargs\n",
    "        Args:\n",
    "            Y: `num_samples x outcome_dim` tensor of outcomes\n",
    "            comps: `num_samples/2 x 2` tensor of pairwise comparisons of Y data\n",
    "            model_kwargs: input transform and covar_module\n",
    "        \"\"\"\n",
    "        util_model = PairwiseGP(\n",
    "            datapoints=Y, comparisons=comps, **model_kwargs)\n",
    "\n",
    "        mll_util = PairwiseLaplaceMarginalLogLikelihood(\n",
    "            util_model.likelihood, util_model)\n",
    "        fit_gpytorch_mll(mll_util)\n",
    "\n",
    "        return util_model\n",
    "\n",
    "    def run_pref_learning(self, method, pe_strategy):\n",
    "\n",
    "        acqf_vals = []\n",
    "        for i in range(self.every_n_comps):\n",
    "\n",
    "            train_Y, train_comps = self.pref_data_dict[method][pe_strategy]\n",
    "\n",
    "            print(\n",
    "                f\"Running {i+1}/{self.every_n_comps} preference learning using {pe_strategy}\")\n",
    "            print(\"train_Y, train_comps shapes: \", train_Y.shape, train_comps.shape)\n",
    "\n",
    "            fit_model_succeed = False\n",
    "\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    pref_model = self.fit_pref_model(\n",
    "                        train_Y,\n",
    "                        train_comps,\n",
    "                        input_transform=self.transforms_covar_dict[method][\"input_tf\"],\n",
    "                        covar_module=self.transforms_covar_dict[method][\"covar_module\"],\n",
    "                        # likelihood=likelihood,\n",
    "                    )\n",
    "                    print(\"Pref model fitting successful\")\n",
    "                    fit_model_succeed = True\n",
    "                    break\n",
    "                except (ValueError, RuntimeError):\n",
    "                    continue\n",
    "            if not fit_model_succeed:\n",
    "                print(\n",
    "                    \"fit_pref_model() failed 3 times, stop current call of run_pref_learn()\"\n",
    "                )\n",
    "\n",
    "            if pe_strategy == \"EUBO-zeta\":\n",
    "                with botorch.settings.debug(state=True):\n",
    "                    # EUBO-zeta\n",
    "                    one_sample_outcome_model = ModifiedFixedSingleSampleModel(\n",
    "                        model=self.outcome_models_dict[method],\n",
    "                        outcome_dim=train_Y.shape[-1]\n",
    "                    ).to(torch.double) # TODO: just added this but didn't help\n",
    "                    acqf = AnalyticExpectedUtilityOfBestOption(\n",
    "                        pref_model=pref_model,\n",
    "                        outcome_model=one_sample_outcome_model\n",
    "                    ).to(torch.double)  # TODO: just added this but didn't help\n",
    "                    found_valid_candidate = False\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            cand_X, acqf_val = optimize_acqf(\n",
    "                                acq_function=acqf,\n",
    "                                q=2,\n",
    "                                bounds=self.problem.bounds,\n",
    "                                num_restarts=self.num_restarts,\n",
    "                                raw_samples=self.raw_samples,  # used for intialization heuristic\n",
    "                                options={\"batch_limit\": 4, \"seed\": self.trial_idx},\n",
    "                            )\n",
    "                            cand_Y = one_sample_outcome_model(cand_X)\n",
    "                            acqf_vals.append(acqf_val.item())\n",
    "\n",
    "                            found_valid_candidate = True\n",
    "                            break\n",
    "                        except (ValueError, RuntimeError) as error:\n",
    "                            print(\"error in optimizing EUBO: \", error)\n",
    "                            continue\n",
    "                    if not found_valid_candidate:\n",
    "                        print(\n",
    "                            f\"optimize_acqf() failed 3 times for EUBO with {method},\", \n",
    "                            \"stop current call of run_pref_learn()\"\n",
    "                        )\n",
    "                        print(\"Current train_Y, train_comps: \", train_Y, train_comps)\n",
    "                        return\n",
    "\n",
    "            elif pe_strategy == \"Random-f\":\n",
    "                # Random-f\n",
    "                cand_X = draw_sobol_samples(\n",
    "                    bounds=self.problem.bounds,\n",
    "                    n=1,\n",
    "                    q=2,\n",
    "                ).squeeze(0).to(torch.double)\n",
    "                cand_Y = self.outcome_models_dict[method].posterior(\n",
    "                    cand_X).rsample().squeeze(0).detach()\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown preference exploration strategy!\")\n",
    "\n",
    "            cand_Y = cand_Y.detach().clone()\n",
    "            cand_comps = gen_comps(self.util_func(cand_Y))\n",
    "\n",
    "            train_comps = torch.cat(\n",
    "                (train_comps, cand_comps + train_Y.shape[0])\n",
    "            )\n",
    "            train_Y = torch.cat((train_Y, cand_Y))\n",
    "            print('train_Y, train_comps shape: ', train_Y.shape, train_comps.shape)\n",
    "\n",
    "            self.pref_data_dict[method][pe_strategy] = (train_Y, train_comps)\n",
    "\n",
    "\n",
    "    def find_max_posterior_mean(self, method, pe_strategy, num_pref_samples=1):\n",
    "\n",
    "        train_Y, train_comps = self.pref_data_dict[method][pe_strategy]\n",
    "\n",
    "        within_result = {}\n",
    "\n",
    "        pref_model = self.fit_pref_model(\n",
    "            Y=train_Y,\n",
    "            comps=train_comps,\n",
    "            input_transform=self.transforms_covar_dict[method][\"input_tf\"],\n",
    "            covar_module=self.transforms_covar_dict[method][\"covar_module\"]\n",
    "        )\n",
    "        sampler = SobolQMCNormalSampler(num_pref_samples)\n",
    "        pref_obj = LearnedObjective(pref_model=pref_model, sampler=sampler)\n",
    "\n",
    "        # find experimental candidate(s) that maximize the posterior mean utility\n",
    "        post_mean_cand_X = gen_exp_cand(\n",
    "            outcome_model=self.outcome_models_dict[method],\n",
    "            objective=pref_obj,\n",
    "            problem=self.problem,\n",
    "            q=1,\n",
    "            acqf_name=\"posterior_mean\",\n",
    "            seed=self.trial_idx\n",
    "        )\n",
    "\n",
    "        post_mean_util = self.util_func(\n",
    "            self.problem.evaluate_true(post_mean_cand_X)).item()\n",
    "        print(\n",
    "            f\"True utility of the current posterior mean utility maximizing design: {post_mean_util:.3f}\")\n",
    "\n",
    "        within_result = {\n",
    "            \"n_comps\": train_comps.shape[0],\n",
    "            \"util\": post_mean_util,\n",
    "            \"run_id\": self.trial_idx,\n",
    "            \"pe_strategy\": pe_strategy,\n",
    "            \"method\": method,\n",
    "        }\n",
    "\n",
    "        return within_result\n",
    "\n",
    "    def generate_final_candidate(self, method, pe_strategy):\n",
    "\n",
    "        train_Y, train_comps = self.pref_data_dict[method][pe_strategy]\n",
    "\n",
    "        pref_model = self.fit_pref_model(\n",
    "            Y=train_Y,\n",
    "            comps=train_comps,\n",
    "            input_transform=self.transforms_covar_dict[method][\"input_tf\"],\n",
    "            covar_module=self.transforms_covar_dict[method][\"covar_module\"]\n",
    "        )\n",
    "\n",
    "        sampler = SobolQMCNormalSampler(1)\n",
    "        pref_obj = LearnedObjective(pref_model=pref_model, sampler=sampler)\n",
    "\n",
    "        # find experimental candidate(s) that maximize the posterior mean utility\n",
    "        cand_X = gen_exp_cand(\n",
    "            outcome_model=self.outcome_models_dict[method],\n",
    "            objective=pref_obj,\n",
    "            problem=self.problem,\n",
    "            q=1,\n",
    "            acqf_name=\"qNEI\",\n",
    "            X=self.X,\n",
    "            seed=self.trial_idx\n",
    "        )\n",
    "\n",
    "        qneiuu_util = self.util_func(self.problem.evaluate_true(cand_X)).item()\n",
    "        print(\n",
    "            f\"{method}-{pe_strategy} qNEIUU candidate utility: {qneiuu_util:.5f}\"\n",
    "        )\n",
    "\n",
    "        exp_result = {\n",
    "            \"candidate\": cand_X,\n",
    "            \"candidate_util\": qneiuu_util,\n",
    "            \"method\": method,\n",
    "            \"strategy\": pe_strategy,\n",
    "            \"run_id\": self.trial_idx,\n",
    "            \"PE_time\": self.PE_time_dict[method][pe_strategy],\n",
    "            # \"util_model_acc\": util_model_acc,\n",
    "        }\n",
    "        \n",
    "        # log the true optimal utility computed in __init__()\n",
    "        exp_result[\"true_opt\"] = self.true_opt\n",
    "\n",
    "        self.final_candidate_results[method][pe_strategy] = exp_result\n",
    "\n",
    "    def generate_random_pref_data(self, method, n):\n",
    "\n",
    "        X = (\n",
    "            draw_sobol_samples(\n",
    "                bounds=self.problem.bounds,\n",
    "                n=1,\n",
    "                q=2*n,\n",
    "                seed=self.trial_idx \n",
    "            )\n",
    "            .squeeze(0)\n",
    "            .to(torch.double)\n",
    "            .detach()\n",
    "        )\n",
    "        Y = self.outcome_models_dict[method].posterior(\n",
    "            X).rsample().squeeze(0).detach()\n",
    "        util = self.util_func(Y)\n",
    "        comps = gen_comps(util)\n",
    "\n",
    "        for pe_strategy in self.pe_strategies:\n",
    "            self.pref_data_dict[method][pe_strategy] = (Y, comps)\n",
    "\n",
    "\n",
    "# ======== Putting it together into steps ========\n",
    "\n",
    "\n",
    "    def run_first_experimentation_stage(self, method):\n",
    "\n",
    "        self.fit_outcome_model(method)\n",
    "\n",
    "\n",
    "    def run_PE_stage(self, method):\n",
    "        # initial result stored in self.pref_data_dict\n",
    "        self.generate_random_pref_data(method, n=1)\n",
    "\n",
    "        for pe_strategy in self.pe_strategies:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(f\"===== Running PE using {method} with {pe_strategy} =====\")\n",
    "\n",
    "            self.PE_session_results[method][pe_strategy] = []\n",
    "\n",
    "            self.PE_session_results[method][pe_strategy].append(\n",
    "                self.find_max_posterior_mean(method, pe_strategy)\n",
    "            )\n",
    "            for j in range(self.n_check_post_mean):\n",
    "                self.run_pref_learning(method, pe_strategy)\n",
    "                self.PE_session_results[method][pe_strategy].append(\n",
    "                    self.find_max_posterior_mean(method, pe_strategy)\n",
    "                )\n",
    "            \n",
    "            # log time required to do PE\n",
    "            PE_time = time.time() - start_time\n",
    "            self.PE_time_dict[method][pe_strategy] = PE_time # will be logged later\n",
    "\n",
    "\n",
    "    def run_second_experimentation_stage(self, method):\n",
    "        for pe_strategy in self.pe_strategies:\n",
    "            print(f\"===== Generating final candidate using {method} with {pe_strategy} =====\")\n",
    "            self.generate_final_candidate(method, pe_strategy)\n",
    "\n",
    "\n",
    "# ======== BOPE loop ========\n",
    "\n",
    "\n",
    "    def run_BOPE_loop(self):\n",
    "        # handle multiple trials\n",
    "        # have a flag for whether the fitting is successful or not\n",
    "\n",
    "        # all methods use the same initial experimentation data\n",
    "        self.generate_random_experiment_data(\n",
    "            self.initial_experimentation_batch,\n",
    "            compute_util=True\n",
    "        )\n",
    "\n",
    "        for method in self.methods:\n",
    "            print(f\"============= Running {method} =============\")\n",
    "            self.run_first_experimentation_stage(method)\n",
    "            self.run_PE_stage(method)\n",
    "            self.run_second_experimentation_stage(method)\n",
    "\n",
    "            if self.save:\n",
    "\n",
    "                torch.save(self.PE_session_results, self.output_path +\n",
    "                        'PE_session_results_trial=' + str(self.trial_idx) + '.th')\n",
    "                torch.save(self.final_candidate_results, self.output_path +\n",
    "                        'final_candidate_results_trial=' + str(self.trial_idx) + '.th')\n",
    "                torch.save(self.outcome_model_fitting_results, self.output_path +\n",
    "                        'outcome_model_fitting_results_trial=' + str(self.trial_idx) + '.th')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yz685/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/test_functions/base.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"bounds\", torch.tensor(self._bounds, dtype=torch.float).transpose(-1, -2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BopeExperiment settings:  {'pca_var_threshold': 0.95, 'initial_experimentation_batch': 16, 'n_check_post_mean': 20, 'every_n_comps': 3, 'verbose': True, 'dtype': torch.float64, 'noise_std': 0.01, 'num_restarts': 20, 'raw_samples': 128, 'batch_limit': 4, 'sampler_num_outcome_samples': 64, 'maxiter': 1000, 'latent_dim': None, 'min_stdv': 100000, 'true_axes': None, 'save': False}\n",
      "self.methods,  ['pca', 'st']\n",
      "============= Running pca =============\n",
      "Fitting outcome model using pca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3146110/1961457152.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.axes_learned = torch.tensor(axes_learned, **tkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of variance explained by 10 axes: 0.9546244571245466\n",
      "===== Running PE using pca with EUBO-zeta =====\n",
      "True utility of the current posterior mean utility maximizing design: 4.000\n",
      "Running 1/3 preference learning using EUBO-zeta\n",
      "train_Y, train_comps shapes:  torch.Size([2, 64]) torch.Size([1, 2])\n",
      "Pref model fitting successful\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "optimize_acqf() failed 3 times for EUBO with pca, stop current call of run_pref_learn()\n",
      "Current train_Y, train_comps:  tensor([[-5.6462e-16,  8.1149e-16,  2.4462e-01,  2.4462e-01,  2.4462e-01,\n",
      "          2.4462e-01,  2.4462e-01,  2.4462e-01, -1.6998e-01, -4.5430e-01,\n",
      "         -1.2804e-01,  1.7116e-01,  1.5858e-01,  1.5858e-01,  1.5858e-01,\n",
      "          6.1361e-01, -1.6998e-01, -4.5430e-01, -1.2804e-01,  1.7116e-01,\n",
      "          1.5858e-01,  1.5858e-01,  1.5858e-01,  6.1361e-01,  0.0000e+00,\n",
      "          1.3133e-01,  1.7019e-01,  3.3806e-01,  3.2548e-01,  3.2548e-01,\n",
      "          3.2548e-01,  5.3897e-01,  4.1071e-01,  4.1071e-01,  6.1946e-01,\n",
      "          2.8654e-01,  2.6459e-01,  1.9629e-01,  2.0567e-01,  3.4137e-01,\n",
      "          4.1071e-01,  1.6980e-01,  3.7855e-01,  4.5630e-02,  2.6459e-01,\n",
      "          1.9629e-01,  2.0567e-01,  3.4137e-01,  6.6179e-01,  1.6980e-01,\n",
      "          2.0867e-01, -1.2426e-01,  9.4700e-02,  2.6402e-02,  3.5782e-02,\n",
      "         -6.8297e-02,  6.6179e-01,  1.6980e-01,  2.0867e-01, -1.2426e-01,\n",
      "          1.0408e-01,  1.0408e-01,  1.0408e-01,  0.0000e+00],\n",
      "        [-1.8140e-16,  2.7049e-16, -4.4636e-01, -4.4636e-01, -4.4636e-01,\n",
      "         -4.4636e-01, -4.4636e-01, -4.4636e-01, -2.9176e-02,  2.3341e-01,\n",
      "         -4.5632e-01, -5.9217e-01, -6.4863e-01, -6.4863e-01, -6.4863e-01,\n",
      "         -5.7548e-01, -2.9176e-02,  2.3341e-01, -4.5632e-01, -5.9217e-01,\n",
      "         -6.4863e-01, -6.4863e-01, -6.4863e-01, -5.7548e-01,  0.0000e+00,\n",
      "          1.8723e-01,  3.3288e-01,  9.8003e-03, -4.6659e-02, -4.6659e-02,\n",
      "         -4.6659e-02, -9.9950e-02,  5.5236e-01,  5.5236e-01,  8.0111e-01,\n",
      "          4.8083e-01,  4.1366e-01,  5.5925e-01,  5.6996e-01,  2.8458e-01,\n",
      "          5.5236e-01,  5.5902e-01,  8.0776e-01,  4.8749e-01,  4.1366e-01,\n",
      "          5.5925e-01,  5.6996e-01,  2.8458e-01,  7.9183e-01,  5.5902e-01,\n",
      "          7.0467e-01,  3.8439e-01,  3.1057e-01,  4.5616e-01,  4.6687e-01,\n",
      "          1.4559e-01,  7.9183e-01,  5.5902e-01,  7.0467e-01,  3.8439e-01,\n",
      "          3.2128e-01,  3.2128e-01,  3.2128e-01,  0.0000e+00]],\n",
      "       dtype=torch.float64) tensor([[0, 1]])\n",
      "True utility of the current posterior mean utility maximizing design: 3.000\n",
      "Running 1/3 preference learning using EUBO-zeta\n",
      "train_Y, train_comps shapes:  torch.Size([2, 64]) torch.Size([1, 2])\n",
      "Pref model fitting successful\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "optimize_acqf() failed 3 times for EUBO with pca, stop current call of run_pref_learn()\n",
      "Current train_Y, train_comps:  tensor([[-5.6462e-16,  8.1149e-16,  2.4462e-01,  2.4462e-01,  2.4462e-01,\n",
      "          2.4462e-01,  2.4462e-01,  2.4462e-01, -1.6998e-01, -4.5430e-01,\n",
      "         -1.2804e-01,  1.7116e-01,  1.5858e-01,  1.5858e-01,  1.5858e-01,\n",
      "          6.1361e-01, -1.6998e-01, -4.5430e-01, -1.2804e-01,  1.7116e-01,\n",
      "          1.5858e-01,  1.5858e-01,  1.5858e-01,  6.1361e-01,  0.0000e+00,\n",
      "          1.3133e-01,  1.7019e-01,  3.3806e-01,  3.2548e-01,  3.2548e-01,\n",
      "          3.2548e-01,  5.3897e-01,  4.1071e-01,  4.1071e-01,  6.1946e-01,\n",
      "          2.8654e-01,  2.6459e-01,  1.9629e-01,  2.0567e-01,  3.4137e-01,\n",
      "          4.1071e-01,  1.6980e-01,  3.7855e-01,  4.5630e-02,  2.6459e-01,\n",
      "          1.9629e-01,  2.0567e-01,  3.4137e-01,  6.6179e-01,  1.6980e-01,\n",
      "          2.0867e-01, -1.2426e-01,  9.4700e-02,  2.6402e-02,  3.5782e-02,\n",
      "         -6.8297e-02,  6.6179e-01,  1.6980e-01,  2.0867e-01, -1.2426e-01,\n",
      "          1.0408e-01,  1.0408e-01,  1.0408e-01,  0.0000e+00],\n",
      "        [-1.8140e-16,  2.7049e-16, -4.4636e-01, -4.4636e-01, -4.4636e-01,\n",
      "         -4.4636e-01, -4.4636e-01, -4.4636e-01, -2.9176e-02,  2.3341e-01,\n",
      "         -4.5632e-01, -5.9217e-01, -6.4863e-01, -6.4863e-01, -6.4863e-01,\n",
      "         -5.7548e-01, -2.9176e-02,  2.3341e-01, -4.5632e-01, -5.9217e-01,\n",
      "         -6.4863e-01, -6.4863e-01, -6.4863e-01, -5.7548e-01,  0.0000e+00,\n",
      "          1.8723e-01,  3.3288e-01,  9.8003e-03, -4.6659e-02, -4.6659e-02,\n",
      "         -4.6659e-02, -9.9950e-02,  5.5236e-01,  5.5236e-01,  8.0111e-01,\n",
      "          4.8083e-01,  4.1366e-01,  5.5925e-01,  5.6996e-01,  2.8458e-01,\n",
      "          5.5236e-01,  5.5902e-01,  8.0776e-01,  4.8749e-01,  4.1366e-01,\n",
      "          5.5925e-01,  5.6996e-01,  2.8458e-01,  7.9183e-01,  5.5902e-01,\n",
      "          7.0467e-01,  3.8439e-01,  3.1057e-01,  4.5616e-01,  4.6687e-01,\n",
      "          1.4559e-01,  7.9183e-01,  5.5902e-01,  7.0467e-01,  3.8439e-01,\n",
      "          3.2128e-01,  3.2128e-01,  3.2128e-01,  0.0000e+00]],\n",
      "       dtype=torch.float64) tensor([[0, 1]])\n",
      "True utility of the current posterior mean utility maximizing design: 4.000\n",
      "Running 1/3 preference learning using EUBO-zeta\n",
      "train_Y, train_comps shapes:  torch.Size([2, 64]) torch.Size([1, 2])\n",
      "Pref model fitting successful\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n",
      "optimize_acqf() failed 3 times for EUBO with pca, stop current call of run_pref_learn()\n",
      "Current train_Y, train_comps:  tensor([[-5.6462e-16,  8.1149e-16,  2.4462e-01,  2.4462e-01,  2.4462e-01,\n",
      "          2.4462e-01,  2.4462e-01,  2.4462e-01, -1.6998e-01, -4.5430e-01,\n",
      "         -1.2804e-01,  1.7116e-01,  1.5858e-01,  1.5858e-01,  1.5858e-01,\n",
      "          6.1361e-01, -1.6998e-01, -4.5430e-01, -1.2804e-01,  1.7116e-01,\n",
      "          1.5858e-01,  1.5858e-01,  1.5858e-01,  6.1361e-01,  0.0000e+00,\n",
      "          1.3133e-01,  1.7019e-01,  3.3806e-01,  3.2548e-01,  3.2548e-01,\n",
      "          3.2548e-01,  5.3897e-01,  4.1071e-01,  4.1071e-01,  6.1946e-01,\n",
      "          2.8654e-01,  2.6459e-01,  1.9629e-01,  2.0567e-01,  3.4137e-01,\n",
      "          4.1071e-01,  1.6980e-01,  3.7855e-01,  4.5630e-02,  2.6459e-01,\n",
      "          1.9629e-01,  2.0567e-01,  3.4137e-01,  6.6179e-01,  1.6980e-01,\n",
      "          2.0867e-01, -1.2426e-01,  9.4700e-02,  2.6402e-02,  3.5782e-02,\n",
      "         -6.8297e-02,  6.6179e-01,  1.6980e-01,  2.0867e-01, -1.2426e-01,\n",
      "          1.0408e-01,  1.0408e-01,  1.0408e-01,  0.0000e+00],\n",
      "        [-1.8140e-16,  2.7049e-16, -4.4636e-01, -4.4636e-01, -4.4636e-01,\n",
      "         -4.4636e-01, -4.4636e-01, -4.4636e-01, -2.9176e-02,  2.3341e-01,\n",
      "         -4.5632e-01, -5.9217e-01, -6.4863e-01, -6.4863e-01, -6.4863e-01,\n",
      "         -5.7548e-01, -2.9176e-02,  2.3341e-01, -4.5632e-01, -5.9217e-01,\n",
      "         -6.4863e-01, -6.4863e-01, -6.4863e-01, -5.7548e-01,  0.0000e+00,\n",
      "          1.8723e-01,  3.3288e-01,  9.8003e-03, -4.6659e-02, -4.6659e-02,\n",
      "         -4.6659e-02, -9.9950e-02,  5.5236e-01,  5.5236e-01,  8.0111e-01,\n",
      "          4.8083e-01,  4.1366e-01,  5.5925e-01,  5.6996e-01,  2.8458e-01,\n",
      "          5.5236e-01,  5.5902e-01,  8.0776e-01,  4.8749e-01,  4.1366e-01,\n",
      "          5.5925e-01,  5.6996e-01,  2.8458e-01,  7.9183e-01,  5.5902e-01,\n",
      "          7.0467e-01,  3.8439e-01,  3.1057e-01,  4.5616e-01,  4.6687e-01,\n",
      "          1.4559e-01,  7.9183e-01,  5.5902e-01,  7.0467e-01,  3.8439e-01,\n",
      "          3.2128e-01,  3.2128e-01,  3.2128e-01,  0.0000e+00]],\n",
      "       dtype=torch.float64) tensor([[0, 1]])\n",
      "True utility of the current posterior mean utility maximizing design: 18.000\n",
      "Running 1/3 preference learning using EUBO-zeta\n",
      "train_Y, train_comps shapes:  torch.Size([2, 64]) torch.Size([1, 2])\n",
      "Pref model fitting successful\n",
      "error in optimizing EUBO:  32 elements of the 32 element gradient array `gradf` are NaN. This often indicates numerical issues. Consider using `dtype=torch.double`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3146110/3821088653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     trial_idx = 0)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_BOPE_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3146110/1687509267.py\u001b[0m in \u001b[0;36mrun_BOPE_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"============= Running {method} =============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_first_experimentation_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_PE_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_second_experimentation_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3146110/1687509267.py\u001b[0m in \u001b[0;36mrun_PE_stage\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_check_post_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pref_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 self.PE_session_results[method][pe_strategy].append(\n\u001b[1;32m    434\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_max_posterior_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3146110/1687509267.py\u001b[0m in \u001b[0;36mrun_pref_learning\u001b[0;34m(self, method, pe_strategy)\u001b[0m\n\u001b[1;32m    251\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             cand_X, acqf_val = optimize_acqf(\n\u001b[0m\u001b[1;32m    254\u001b[0m                                 \u001b[0macq_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macqf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/optim/optimize.py\u001b[0m in \u001b[0;36moptimize_acqf\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, sequential, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_conditions_provided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         batch_initial_conditions = ic_gen(\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0macq_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macq_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/optim/initializers.py\u001b[0m in \u001b[0;36mgen_batch_initial_conditions\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mX_rnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_rnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                     Y_rnd_curr = acq_function(\n\u001b[0m\u001b[1;32m    194\u001b[0m                         \u001b[0mX_rnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     ).cpu()\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/utils/transforms.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;31m# add t-batch dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macqf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macqf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_fully_bayesian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macqf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/acquisition/preference.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_winner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3146110/2918881069.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mpost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/models/gpytorch.py\u001b[0m in \u001b[0;36mposterior\u001b[0;34m(self, X, output_indices, observation_noise, posterior_transform, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mcovar_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_covariance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0moutput_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_indices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 mvns = [\n\u001b[0m\u001b[1;32m    393\u001b[0m                     MultivariateNormal(\n\u001b[1;32m    394\u001b[0m                         \u001b[0mmean_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dim_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/botorch/models/gpytorch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0moutput_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_indices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 mvns = [\n\u001b[0;32m--> 393\u001b[0;31m                     MultivariateNormal(\n\u001b[0m\u001b[1;32m    394\u001b[0m                         \u001b[0mmean_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dim_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                         \u001b[0mcovar_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutput_dim_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/pyro/distributions/distribution.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mean, covariance_matrix, validate_args)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__unbroadcasted_scale_tril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_shapes\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# This wrapper exists to support variadic args.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# TODO Move this to C++ once the jit has better support for torch.Size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bope_pca/lib/python3.9/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mis_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1020\u001b[0m     code with ``torch.jit.trace``) and ``False`` otherwise.\n\u001b[1;32m   1021\u001b[0m     \"\"\"\n\u001b[0;32m-> 1022\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "problem = Image(num_pixels=8)\n",
    "util_func = AreaUtil()\n",
    "\n",
    "experiment = BopeExperiment(\n",
    "    problem, \n",
    "    util_func, \n",
    "    methods = [\"st\", \"pca\"],\n",
    "    pe_strategies = [\"EUBO-zeta\", \"Random-f\"],\n",
    "    trial_idx = 0)\n",
    "\n",
    "experiment.run_BOPE_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bope_pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f178f7686bb85c5c6e141a85fd4c17c3082d63b89f6cfaecdf98c22c0047a219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
